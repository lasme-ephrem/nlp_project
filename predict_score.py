# -*- coding: utf-8 -*-
"""predict_score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YuLNd6q8pHyu_n989I_oA2TlSxnU7Z-5
"""

!pip install transformers
!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

import datasets
from datasets import Dataset
from transformers import AutoModelForSequenceClassification
import numpy as np
from datasets import load_metric
from transformers import TrainingArguments, Trainer

import os
import json
import pandas as pd
BASE_DIR = "/content/drive/MyDrive/NLP_ENSAE/"
json_name = "Digital_Music.json"
os.chdir(BASE_DIR)
df = pd.read_json(os.path.join(BASE_DIR, json_name), lines = True)
df = df.drop(['image', 'reviewerID', 'asin'], axis = 1)
df.dropna(inplace=True)
df.reset_index(inplace=True)

# def get_dataset_format(row_df):
#     return {'label' : row_df['overall'] , 'text' : row_df['reviewText'] }

# df['dataset_format'] = df.apply(get_dataset_format, axis = 1)

train_df = pd.DataFrame({
     
     "label" : df[:int(0.8*df.shape[0])]['overall'].tolist(),
     "text" : df[:int(0.8*df.shape[0])]['reviewText'].tolist()
})

test_df = pd.DataFrame({
     "label" : df[int(0.8*df.shape[0]):]['overall'].tolist(),
     "text" : df[int(0.8*df.shape[0]):]['reviewText'].tolist()
})

# dataset_ = df['dataset_format']
# dataset_ = pd.DataFrame(dataset_, columns=['dataset_format'])
# #shuffle dataset
# dataset_ = dataset_.sample(frac = 1)
# train_df = dataset_[:int(0.8*dataset_.shape[0])]
# test_df = dataset_[int(0.8*dataset_.shape[0]):]

train_df.head()

from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df).select(range(1000))
test_dataset = Dataset.from_pandas(test_df).select(range(1000))

my_dataset_dict = datasets.DatasetDict({"train":train_dataset,"test":test_dataset})

type(my_dataset_dict)

my_dataset_dict.keys()

dataset = my_dataset_dict

dataset['train'][10]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True, batch_size = 8)

small_train_dataset = tokenized_datasets["train"]
small_eval_dataset = tokenized_datasets["test"]

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-cased", num_labels=6)

#sum(p.numel() for p in model.parameters()) #Bert based case

sum(p.numel() for p in model.parameters())

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer", do_train = True, 
                                  num_train_epochs = 25,
                                  per_device_train_batch_size = 8, )

import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch", do_train = True,
                                  num_train_epochs = 25,
                                  per_device_train_batch_size = 8,)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()