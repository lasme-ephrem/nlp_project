{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da7ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gestion des exceptions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a17019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lasme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#gestion des répertoires\n",
    "import os\n",
    "\n",
    "#pour le chargement des données depuis notre espace S3 AWS\n",
    "import boto3\n",
    "\n",
    "#modules de base pour la science des données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#gestion et preprocessing des données textuelles\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "#pour la réalisation des nuages de mots\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#pour l'entraînement réseau de neuronnes artificielles pour le sentiment analysis\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch.optim as optim\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer #Tokenizer pour bert\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "#pour la séparation des données afin d'estimer l'erreur de généralisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#pour le calcul de la métrique utilisée pour évaluer le classifieur pour le sentiment analysis\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#pour la bar de progression\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e5c2c",
   "metadata": {},
   "source": [
    "### Transfert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065356a",
   "metadata": {},
   "source": [
    "Peut-on employer les modèles que nous avons appris pour l'analyse de sentiments sur les produits de la categorie \"musique digital\" à d'autres categories de produits sur Amazon ? Dans cette partie, nous appliquons notre LSTM pour la prédiction de sentiments sur quelques instances ou commentaires de la catégorie de produits \"livres\" pour laquelle il n'a pas été entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c453da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size) # fully connected\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long() # cast to long tensor\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610c9653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(13688, 400)\n",
      "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 13688\n",
    "output_size = 1\n",
    "embedding_dim = 400 \n",
    "hidden_dim = 512 \n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "model.load_state_dict(torch.load('best_LSTM_model.pt'))\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9fb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Movies_and_TV_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "606207e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Movies_TV.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60ad71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2c21fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Movies_TV_low.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bba05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1312137</th>\n",
       "      <td>5.0</td>\n",
       "      <td>04 14, 2015</td>\n",
       "      <td>We love this whole series. Well made by the Hi...</td>\n",
       "      <td>Well Made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345026</th>\n",
       "      <td>5.0</td>\n",
       "      <td>01 8, 2016</td>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833505</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12 27, 2010</td>\n",
       "      <td>The first time I saw this, I thought, \"meh.\"  ...</td>\n",
       "      <td>Not as bad as people claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553086</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 30, 2016</td>\n",
       "      <td>Very clear copy. Well worth the price.</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519385</th>\n",
       "      <td>3.0</td>\n",
       "      <td>09 4, 2014</td>\n",
       "      <td>Good movie, but...some what predictable @ cert...</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         overall   reviewTime  \\\n",
       "1312137      5.0  04 14, 2015   \n",
       "3345026      5.0   01 8, 2016   \n",
       "833505       4.0  12 27, 2010   \n",
       "553086       5.0  11 30, 2016   \n",
       "2519385      3.0   09 4, 2014   \n",
       "\n",
       "                                                reviewText  \\\n",
       "1312137  We love this whole series. Well made by the Hi...   \n",
       "3345026                                             Great!   \n",
       "833505   The first time I saw this, I thought, \"meh.\"  ...   \n",
       "553086              Very clear copy. Well worth the price.   \n",
       "2519385  Good movie, but...some what predictable @ cert...   \n",
       "\n",
       "                            summary  \n",
       "1312137                   Well Made  \n",
       "3345026                  Five Stars  \n",
       "833505   Not as bad as people claim  \n",
       "553086                   Five Stars  \n",
       "2519385                 Three Stars  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = df.drop(labels = [\"verified\", \"reviewerID\", \"asin\" ,\"style\",\"reviewerName\", \"unixReviewTime\",\"vote\", \"image\"],axis = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f825248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = [words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct =''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "\n",
    "df[\"reviewText\"] = df[\"reviewText\"].astype(str)\n",
    "df[\"reviewText\"] = df[\"reviewText\"].apply(lambda x: remove_punctuation(x.lower()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8fdba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitement des données en cours, patientez encore ...\n",
      "===============================================================\n",
      "...\n",
      "...\n",
      "...\n",
      "Prétraitement terminé !\n"
     ]
    }
   ],
   "source": [
    "def text_process(df):\n",
    "    # racinisation\n",
    "    stemmer = PorterStemmer()\n",
    "    # élimination des stop words anglais comme \"the, I, our etc\"\n",
    "    words = stopwords.words(\"english\") \n",
    "    df['cleaned_reviews'] = df['reviewText'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "    print(\"Prétraitement des données en cours, patientez encore ...\")\n",
    "    print(\"===============================================================\")\n",
    "    print(\"...\")\n",
    "    print(\"...\")\n",
    "    print(\"...\")\n",
    "    print(\"Prétraitement terminé !\")\n",
    "    return df\n",
    "\n",
    "df = text_process(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "812df242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    if x == 5.0 or x == 4.0:\n",
    "        return 2 #positif\n",
    "    if x == 3.0:\n",
    "        return 1 #neutre\n",
    "    return 0 #négatif\n",
    "\n",
    "df[\"y\"] = df[\"overall\"].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e69b4c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    778\n",
       "0    222\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(x):\n",
    "    if x == 0 or x == 1:\n",
    "        return 0 #négatif\n",
    "    return 1 #positif\n",
    "\n",
    "df[\"y\"] = df[\"y\"].apply(classify)\n",
    "df[\"y\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47a16488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced=df\n",
    "\n",
    "df_balanced[\"cleaned_reviews\"] = df_balanced[\"cleaned_reviews\"].astype(\"str\")\n",
    "all_words = [w for s in df_balanced.cleaned_reviews.values.tolist() for w in s.split()]\n",
    "words = Counter(all_words)\n",
    "words = {k:v for k,v in words.items() if v > 1}\n",
    "words = sorted(words, key = words.get, reverse = True)\n",
    "words = ['_PAD', '_UNK'] + words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a80a8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = { w : i for i,w in enumerate(words) }\n",
    "idx_to_word = {v : k for k,v in word_to_idx.items()}\n",
    "\n",
    "def process_sequence(text_sequence):\n",
    "    global word_to_idx\n",
    "    res = []\n",
    "    for w in text_sequence:\n",
    "        if w in word_to_idx:\n",
    "            res.append(word_to_idx[w])\n",
    "        else:\n",
    "            res.append(1)\n",
    "    return res\n",
    "\n",
    "X = [process_sequence(s.split()) for s in df_balanced.cleaned_reviews.values.tolist()]\n",
    "y = df_balanced.y.values.tolist()\n",
    "\n",
    "#padding des données\n",
    "MAX_LEN = max([len(s) for s in  X])\n",
    "X = pad_sequences(X, maxlen = MAX_LEN, padding = 'pre')\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02411276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formattage des données train et test pour pytorch\n",
    "test_data = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "###taille du batch\n",
    "batch_size = 32 #gérer la taille du batch en fonction de ses capacités GPU, chez le max c'est 32 pour cette architecture\n",
    "\n",
    "test_loader = DataLoader(test_data, shuffle = True, batch_size = batch_size, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ae4a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "criterion = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d386652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.866\n",
      "Test accuracy: 56.000%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze()) #rounds the output to 0/1´\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    y_pred.extend(pred.tolist())\n",
    "    y_true.extend(labels.tolist())\n",
    "        \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0647f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
